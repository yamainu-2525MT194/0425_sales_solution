# --- ãƒ•ã‚¡ã‚¤ãƒ«å: app.pyï¼ˆOpenAI v1 å¯¾å¿œ + GPT-3.5 ä½¿ç”¨ç‰ˆï¼‰

import streamlit as st
import pandas as pd
import numpy as np
import json
import re
from openai import OpenAI
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, classification_report
from lightgbm import LGBMRegressor
from google.oauth2 import service_account
from googleapiclient.discovery import build

st.set_page_config(page_title="å–¶æ¥­AIåˆ†æãƒ„ãƒ¼ãƒ«ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å¯¾å¿œï¼‰", layout="wide")
st.title("ğŸ“Š å–¶æ¥­AIåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆCSV or Google Docså¯¾å¿œï¼‰")

# --- èªè¨¼æƒ…å ±ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ« .streamlit/secrets.tomlï¼‰ ---
credentials_dict = json.loads(st.secrets["gspread_credentials"])
credentials = service_account.Credentials.from_service_account_info(
    credentials_dict,
    scopes=["https://www.googleapis.com/auth/drive", "https://www.googleapis.com/auth/documents.readonly"]
)
client_openai = OpenAI(api_key=st.secrets["openai_api_key"])

# --- å…¥åŠ›å½¢å¼ã®é¸æŠ ---
input_type = st.radio("ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡ã‚’é¸æŠ", ["Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼ˆCSVï¼‰", "Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆæ—¥å ±å½¢å¼ï¼‰"])

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
df = None
free_text = ""

if input_type == "Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼ˆCSVï¼‰":
    sheet_url = st.text_input("ğŸ“ Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼ˆCSVãƒªãƒ³ã‚¯ï¼‰")
    if sheet_url and "export?format=csv" in sheet_url:
        try:
            df = pd.read_csv(sheet_url)
            df["æ—¥ä»˜"] = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")
            start_date = st.date_input("é–‹å§‹æ—¥", df["æ—¥ä»˜"].min().date())
            end_date = st.date_input("çµ‚äº†æ—¥", df["æ—¥ä»˜"].max().date())
            df = df[(df["æ—¥ä»˜"] >= pd.to_datetime(start_date)) & (df["æ—¥ä»˜"] <= pd.to_datetime(end_date))]
            st.success(f"âœ… ãƒ‡ãƒ¼ã‚¿ {len(df)} ä»¶ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        except Exception as e:
            st.error(f"CSVã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼š{e}")

elif input_type == "Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆæ—¥å ±å½¢å¼ï¼‰":
    doc_input = st.text_input("ğŸ“ Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID ã¾ãŸã¯ URL ã‚’å…¥åŠ›")
    match = re.search(r'/d/([a-zA-Z0-9-_]+)', doc_input)
    doc_id = match.group(1) if match else doc_input.strip()

    if doc_id:
        try:
            service = build('docs', 'v1', credentials=credentials)
            doc = service.documents().get(documentId=doc_id).execute()
            content = doc.get("body", {}).get("content", [])
            for c in content:
                for e in c.get("paragraph", {}).get("elements", []):
                    free_text += e.get("textRun", {}).get("content", "")
            st.text_area("ğŸ“„ æŠ½å‡ºã•ã‚ŒãŸæœ¬æ–‡ï¼ˆå…ˆé ­ï¼‰", free_text[:1000])
        except Exception as e:
            st.error(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼ï¼š{e}")

# --- æ•°å€¤ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆCSVã®ã¿ï¼‰ ---
if df is not None:
    st.header("ğŸ“ˆ æ•°å€¤ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆCSVï¼‰")
    try:
        X = df[["è·é›»æ•°", "ææ¡ˆæ•°_æœ‰åŠ¹"]]
        y = df["é¢è«‡ç²å¾—æ•°"]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        rf_model = RandomForestRegressor(n_estimators=100).fit(X_train, y_train)
        st.subheader("ğŸ“‰ é¢è«‡æ•°äºˆæ¸¬ï¼ˆRandom Forestï¼‰")
        st.write(f"RMSE: {np.sqrt(mean_squared_error(y_test, rf_model.predict(X_test))):.2f}")

        y_binary = (df["ææ¡ˆæ•°_æœ‰åŠ¹"] > df["ææ¡ˆæ•°_æœ‰åŠ¹"].median()).astype(int)
        log_model = LogisticRegression().fit(X_train, y_binary)
        st.subheader("ğŸ“Š ææ¡ˆåŠ›ã®åˆ†é¡ï¼ˆãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼‰")
        st.text(classification_report(y_binary, log_model.predict(X_train)))

        kmeans = KMeans(n_clusters=3).fit(X)
        df["å–¶æ¥­ã‚¿ã‚¤ãƒ—"] = kmeans.labels_
        st.subheader("ğŸ” å–¶æ¥­ã‚¿ã‚¤ãƒ—ã®åˆ†é¡ï¼ˆKMeansï¼‰")
        st.dataframe(df[["å–¶æ¥­å", "å–¶æ¥­ã‚¿ã‚¤ãƒ—"]].drop_duplicates())

    except Exception as e:
        st.error(f"æ•°å€¤åˆ†æä¸­ã®ã‚¨ãƒ©ãƒ¼ï¼š{e}")

# --- GPTè¦ç´„ãƒ»ææ¡ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå…±é€šï¼‰ ---
st.header("ğŸ’¬ GPTã«ã‚ˆã‚‹AIè¦ç´„ãƒ»ã‚¢ãƒ‰ãƒã‚¤ã‚¹")

if df is not None:
    try:
        issues = "\n".join(df["èª²é¡Œ"].dropna().astype(str))
        good = "\n".join(df["è‰¯ã‹ã£ãŸç‚¹"].dropna().astype(str))
        counter = "\n".join(df["èª²é¡Œã‚’è§£æ±ºã™ã‚‹ç‚ºã®å¯¾ç­–"].dropna().astype(str))

        prompt = f"ä»¥ä¸‹ã®å–¶æ¥­å ±å‘Šã‹ã‚‰ã€æˆåŠŸè¦å› ãƒ»èª²é¡Œãƒ»å¯¾ç­–ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚\nè‰¯ã‹ã£ãŸç‚¹:\n{good}\nèª²é¡Œ:\n{issues}\nå¯¾ç­–æ¡ˆ:\n{counter}"

        result = client_openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        st.markdown(result.choices[0].message.content)

    except Exception as e:
        st.error(f"GPTåˆ†æä¸­ã®ã‚¨ãƒ©ãƒ¼ï¼š{e}")

elif free_text:
    prompt = f"ä»¥ä¸‹ã®å–¶æ¥­æ—¥å ±ã‚’åˆ†æã—ã€æ•°å€¤æˆæœãƒ»é¢è«‡æ‰€æ„Ÿãƒ»é¡§å®¢æ‰€æ„Ÿãƒ»èª²é¡Œãƒ»å¯¾ç­–ã‚’ã¾ã¨ã‚ã€AIã‹ã‚‰ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚‚è¿½åŠ ã—ã¦ãã ã•ã„ï¼š\n{free_text[:3500]}"
    try:
        result = client_openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        st.markdown(result.choices[0].message.content)
    except Exception as e:
        st.error(f"GPTå‡¦ç†ã‚¨ãƒ©ãƒ¼ï¼š{e}")
else:
    st.info("å·¦ã‹ã‚‰CSVã¾ãŸã¯Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’é¸æŠã—ã€ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")
