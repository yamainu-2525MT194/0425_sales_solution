# --- ãƒ•ã‚¡ã‚¤ãƒ«å: app.pyï¼ˆOpenAI v1 å¯¾å¿œ + GPT-3.5 ä½¿ç”¨ç‰ˆï¼‰


import streamlit as st
import pandas as pd
import numpy as np
import re
import json
from openai import OpenAI
from sklearn.linear_model import LinearRegression
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from google.oauth2 import service_account
from googleapiclient.discovery import build
import gspread
from gspread_dataframe import set_with_dataframe
import matplotlib.pyplot as plt
import seaborn as sns
from io import StringIO

st.set_page_config(page_title="å–¶æ¥­æ—¥å ±AIè§£æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆæ‹¡å¼µ300è¡Œï¼‰", layout="wide")
st.title("ğŸ“Š å–¶æ¥­æ—¥å ±AIè§£æãƒ„ãƒ¼ãƒ«ï¼ˆå…¨æ–¹ä½æ‹¡å¼µï¼‰")

# --- èªè¨¼è¨­å®š ---
credentials_dict = json.loads(st.secrets["gspread_credentials"])
credentials = service_account.Credentials.from_service_account_info(
    credentials_dict,
    scopes=["https://www.googleapis.com/auth/documents.readonly", "https://www.googleapis.com/auth/spreadsheets"]
)
client_openai = OpenAI(api_key=st.secrets["openai_api_key"])

# --- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¥åŠ› ---
doc_input = st.text_input("ğŸ“„ Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID ã¾ãŸã¯ URL")
doc_id_match = re.search(r'/d/([a-zA-Z0-9-_]+)', doc_input)
doc_id = doc_id_match.group(1) if doc_id_match else doc_input.strip()

if doc_id:
    try:
        service = build('docs', 'v1', credentials=credentials)
        doc = service.documents().get(documentId=doc_id).execute()
        content = doc.get("body", {}).get("content", [])

        full_text = ""
        for c in content:
            for e in c.get("paragraph", {}).get("elements", []):
                full_text += e.get("textRun", {}).get("content", "")

        st.text_area("ğŸ“‹ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ï¼ˆå…ˆé ­ï¼‰", full_text[:1000], height=200)

        # --- è¤‡æ•°äººã®æŠ½å‡ºå‡¦ç† ---
        sections = re.split(r"åå‰ï¼š(.+?)\n", full_text)
        reports = []

        for i in range(1, len(sections), 2):
            name = sections[i].strip()
            text = sections[i + 1] if i + 1 < len(sections) else ""
            data = {"åå‰": name}
            matches = re.findall(r"(ã‚ªãƒ•ã‚¡ãƒ¼æ•°|é¢è«‡ç²å¾—æ•°|ææ¡ˆæ•°ï¼ˆæœ‰åŠ¹ï¼‰|ææ¡ˆæ•°ï¼ˆæœ‰åŠ¹ãƒ»ç„¡åŠ¹ãƒ»ä¸æ˜ï¼‰|é…ä¿¡æ•°|è·é›»æ•°|é¢è«‡å®Ÿæ–½æ•°)ï¼š([0-9]+)ä»¶", text)
            for label, value in matches:
                label = label.replace("ï¼ˆæœ‰åŠ¹ï¼‰", "_æœ‰åŠ¹").replace("ï¼ˆæœ‰åŠ¹ãƒ»ç„¡åŠ¹ãƒ»ä¸æ˜ï¼‰", "_åˆè¨ˆ")
                data[label] = int(value)
            reports.append(data)

        df = pd.DataFrame(reports).fillna(0)
        st.dataframe(df)

        # --- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ & å‡ºåŠ› ---
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button("ğŸ“¥ CSVã¨ã—ã¦ä¿å­˜", csv, "report_data.csv", "text/csv")

        sheet_url = st.text_input("ğŸ“ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURLï¼ˆè¿½è¨˜å…ˆï¼‰")
        if sheet_url:
            try:
                gc = gspread.authorize(credentials)
                sheet_id = re.search(r"/d/([\w-]+)", sheet_url).group(1)
                sh = gc.open_by_key(sheet_id).sheet1
                set_with_dataframe(sh, df, row=sh.row_count + 1)
                st.success("âœ… è¿½è¨˜å®Œäº†")
            except Exception as e:
                st.warning(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ï¼š{e}")

        # --- åˆ†æãƒ»å¯è¦–åŒ– ---
        st.subheader("ğŸ“ˆ é¢è«‡æ•°å›å¸°äºˆæ¸¬")
        if 'é¢è«‡ç²å¾—æ•°' in df.columns:
            features = df.drop(columns=['é¢è«‡ç²å¾—æ•°', 'åå‰'], errors='ignore').fillna(0)
            model = LinearRegression().fit(features, df['é¢è«‡ç²å¾—æ•°'])
            df['äºˆæ¸¬é¢è«‡æ•°'] = model.predict(features)
            st.dataframe(df[['åå‰', 'é¢è«‡ç²å¾—æ•°', 'äºˆæ¸¬é¢è«‡æ•°']])

        st.subheader("ğŸ” ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° + PCA å¯è¦–åŒ–")
        scaler = StandardScaler()
        scaled = scaler.fit_transform(df.drop(columns=['åå‰'], errors='ignore'))
        kmeans = KMeans(n_clusters=2, n_init='auto').fit(scaled)
        df['å–¶æ¥­ã‚¿ã‚¤ãƒ—'] = kmeans.labels_

        pca = PCA(n_components=2)
        components = pca.fit_transform(scaled)
        pca_df = pd.DataFrame(components, columns=['PC1', 'PC2'])
        pca_df['åå‰'] = df['åå‰']
        pca_df['å–¶æ¥­ã‚¿ã‚¤ãƒ—'] = df['å–¶æ¥­ã‚¿ã‚¤ãƒ—']

        fig, ax = plt.subplots()
        sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='å–¶æ¥­ã‚¿ã‚¤ãƒ—', style='åå‰', ax=ax)
        st.pyplot(fig)

        st.subheader("ğŸ“Š ç›¸é–¢ãƒãƒˆãƒªã‚¯ã‚¹")
        corr = df.drop(columns=['åå‰'], errors='ignore').corr()
        fig2, ax2 = plt.subplots(figsize=(8, 6))
        sns.heatmap(corr, annot=True, fmt=".2f", cmap="Blues", ax=ax2)
        st.pyplot(fig2)

        st.subheader("ğŸ§  GPTï¼šè¦ç´„ãƒ»æ”¹å–„ãƒ»ã‚¤ãƒ³ã‚µã‚¤ãƒˆæŠ½å‡º")
        for i in range(1, len(sections), 2):
            name = sections[i].strip()
            content = sections[i + 1][:1500]
            prompt = f"ä»¥ä¸‹ã¯{name}ã•ã‚“ã®å–¶æ¥­æ—¥å ±ã§ã™ã€‚æˆæœã€èª²é¡Œã€å¯¾ç­–ã€å·¥å¤«ç‚¹ã€ãã—ã¦æ”¹å–„æ¡ˆã‚’æ•´ç†ã—ã¦ãã ã•ã„ã€‚\n{content}"
            response = client_openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            st.markdown(f"### {name} ã•ã‚“ã¸ã®AIåˆ†æ")
            st.markdown(response.choices[0].message.content)

        # --- è¿½åŠ æ©Ÿèƒ½ï¼šç›®æ¨™ã¨ã®ä¹–é›¢ãƒã‚§ãƒƒã‚¯ ---
        st.subheader("ğŸ¯ å„å–¶æ¥­ã®ç›®æ¨™é”æˆåº¦åˆ†æ")
        target_offer = st.number_input("ç›®æ¨™ã‚ªãƒ•ã‚¡ãƒ¼æ•°ï¼ˆå…¨å–¶æ¥­å…±é€šï¼‰", value=3)
        if 'ã‚ªãƒ•ã‚¡ãƒ¼æ•°' in df.columns:
            df['ã‚ªãƒ•ã‚¡ãƒ¼é”æˆç‡'] = df['ã‚ªãƒ•ã‚¡ãƒ¼æ•°'] / target_offer
            fig3, ax3 = plt.subplots()
            sns.barplot(data=df, x='åå‰', y='ã‚ªãƒ•ã‚¡ãƒ¼é”æˆç‡', ax=ax3)
            ax3.axhline(1, color='red', linestyle='--')
            st.pyplot(fig3)

    except Exception as e:
        st.error(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼ï¼š{e}")
else:
    st.info("Googleãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆURLã¾ãŸã¯IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
