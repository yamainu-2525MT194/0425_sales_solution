# --- ãƒ•ã‚¡ã‚¤ãƒ«å: app.pyï¼ˆOpenAI v1 å¯¾å¿œ + GPT-3.5 ä½¿ç”¨ç‰ˆï¼‰

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, classification_report
from lightgbm import LGBMRegressor
from openai import OpenAI
import gspread
import json
from oauth2client.service_account import ServiceAccountCredentials
import datetime

# --- Googleèªè¨¼ï¼ˆsecretsçµŒç”±ã«ä¿®æ­£ï¼‰ ---
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
credentials_dict = json.loads(st.secrets["gspread_credentials"])
credentials = ServiceAccountCredentials.from_json_keyfile_dict(credentials_dict, scope)
client_gs = gspread.authorize(credentials)

# --- Streamlit UIè¨­å®š ---
st.set_page_config(page_title="AIå–¶æ¥­åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", layout="wide")
st.title("ğŸ§  AIå–¶æ¥­åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆLightGBM Ã— GPTã«ã‚ˆã‚‹å®Œå…¨æ”¯æ´ï¼‰")

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---
st.sidebar.header("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿è¨­å®š")
sheet_url = st.sidebar.text_input("Google Sheetsã®CSVãƒªãƒ³ã‚¯ï¼ˆexport?format=csvï¼‰")

if sheet_url and "export?format=csv" in sheet_url:
    try:
        df = pd.read_csv(sheet_url)
        df["æ—¥ä»˜"] = pd.to_datetime(df["æ—¥ä»˜"], errors="coerce")

        start_date = st.sidebar.date_input("é–‹å§‹æ—¥", df["æ—¥ä»˜"].min().date())
        end_date = st.sidebar.date_input("çµ‚äº†æ—¥", df["æ—¥ä»˜"].max().date())
        df = df[(df["æ—¥ä»˜"] >= pd.to_datetime(start_date)) & (df["æ—¥ä»˜"] <= pd.to_datetime(end_date))]

        st.success(f"âœ… ãƒ‡ãƒ¼ã‚¿ {len(df)} ä»¶ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

        # --- æ©Ÿæ¢°å­¦ç¿’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
        st.header("ğŸ¤– æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹å–¶æ¥­åˆ†æ")
        X = df[["è·é›»æ•°", "ææ¡ˆæ•°_æœ‰åŠ¹"]]
        y = df["é¢è«‡ç²å¾—æ•°"]

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°
        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_model.fit(X_train, y_train)
        y_pred_rf = rf_model.predict(X_test)
        st.subheader("ğŸ“ˆ é¢è«‡æ•°äºˆæ¸¬ï¼šRandom Forest")
        st.write(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_rf)):.2f}")

        # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆææ¡ˆåŠ›ï¼‰
        st.subheader("ğŸ“‰ ææ¡ˆåŠ›åˆ†é¡ï¼šLogistic Regression")
        y_binary = (df["ææ¡ˆæ•°_æœ‰åŠ¹"] > df["ææ¡ˆæ•°_æœ‰åŠ¹"].median()).astype(int)
        X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y_binary, test_size=0.2, random_state=42)
        log_model = LogisticRegression().fit(X_train2, y_train2)
        y_pred2 = log_model.predict(X_test2)
        st.text(classification_report(y_test2, y_pred2))

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        st.subheader("ğŸ” å–¶æ¥­ã‚¿ã‚¤ãƒ—åˆ†é¡ï¼šKMeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°")
        kmeans = KMeans(n_clusters=3, random_state=42).fit(X)
        df["å–¶æ¥­ã‚¿ã‚¤ãƒ—"] = kmeans.labels_
        st.dataframe(df[["å–¶æ¥­å", "å–¶æ¥­ã‚¿ã‚¤ãƒ—"]].drop_duplicates())

        # ç‰¹å¾´é‡ã®é‡è¦åº¦
        st.subheader("ğŸ“Š æˆæœã«å¯„ä¸ã™ã‚‹æŒ‡æ¨™")
        importance = pd.Series(rf_model.feature_importances_, index=X.columns)
        st.bar_chart(importance)

        # LightGBMäºˆæ¸¬
        st.subheader("ğŸš€ é«˜ç²¾åº¦äºˆæ¸¬ï¼šLightGBM")
        lgb_model = LGBMRegressor(n_estimators=100)
        lgb_model.fit(X_train, y_train)
        y_pred_lgb = lgb_model.predict(X_test)
        st.line_chart(pd.DataFrame({"LGBäºˆæ¸¬": y_pred_lgb, "å®Ÿæ¸¬": y_test.values}))

        # --- GPTæˆ¦ç•¥åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
        st.header("ğŸ’¬ GPTã«ã‚ˆã‚‹æˆ¦ç•¥ã‚¢ãƒ‰ãƒã‚¤ã‚¹")
        client = OpenAI(api_key=st.secrets["openai_api_key"])

        issues = "\n".join(df["èª²é¡Œ"].dropna().astype(str))
        good_points = "\n".join(df["è‰¯ã‹ã£ãŸç‚¹"].dropna().astype(str))
        solutions = "\n".join(df["å¯¾ç­–"].dropna().astype(str))

        st.subheader("ğŸ§  èª²é¡Œã®å‚¾å‘åˆ†æ")
        prompt1 = f"ä»¥ä¸‹ã®å–¶æ¥­èª²é¡Œã‹ã‚‰å…±é€šã™ã‚‹å‚¾å‘ã¨é »å‡ºãƒ†ãƒ¼ãƒã‚’3ã¤è¦ç´„ã—ã¦ãã ã•ã„:\n{issues}"
        st.markdown(client.chat.completions.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": prompt1}]).choices[0].message.content)

        st.subheader("âœ¨ æˆåŠŸè¦å› ã®æŠ½å‡º")
        prompt2 = f"ä»¥ä¸‹ã®ã€è‰¯ã‹ã£ãŸç‚¹ã€ã‹ã‚‰ã€å–¶æ¥­ãƒãƒ¼ãƒ ã«ã¨ã£ã¦ã®æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’3ã¤æŠ½å‡ºã—ã¦ãã ã•ã„:\n{good_points}"
        st.markdown(client.chat.completions.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": prompt2}]).choices[0].message.content)

        st.subheader("ğŸ”§ å¯¾ç­–æ¡ˆã®åŠ¹æœã¨æ”¹å–„")
        prompt3 = f"ä»¥ä¸‹ã®å¯¾ç­–æ¡ˆã‚’èª­ã¿ã€åŠ¹æœãŒé«˜ã„é †ã«3ã¤ã¨æ”¹å–„ææ¡ˆã‚’æ•™ãˆã¦ãã ã•ã„:\n{solutions}"
        st.markdown(client.chat.completions.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": prompt3}]).choices[0].message.content)

        st.subheader("ğŸ“† ä»Šæœˆã®å‚¾å‘è¦ç´„")
        recent_prompt = f"ä»¥ä¸‹ã®èª²é¡Œã¨æˆæœå†…å®¹ã‹ã‚‰ã€ä»Šæœˆã®å–¶æ¥­æ´»å‹•ã®å…¨ä½“å‚¾å‘ã‚’è¦ç´„ã—ã¦ãã ã•ã„:\n{issues}\n{good_points}"
        st.markdown(client.chat.completions.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": recent_prompt}]).choices[0].message.content)

        st.subheader("ğŸ“Œ å–¶æ¥­åˆ¥AIã‚¢ãƒ‰ãƒã‚¤ã‚¹")
        for name in df["å–¶æ¥­å"].unique():
            personal_data = df[df["å–¶æ¥­å"] == name]
            prompt5 = f"{name}ã•ã‚“ã®å–¶æ¥­æ´»å‹•ãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¨ã«ã€å€‹åˆ¥ã«æ”¹å–„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’å‡ºã—ã¦ãã ã•ã„:\n{personal_data[['è·é›»æ•°', 'é¢è«‡ç²å¾—æ•°', 'ææ¡ˆæ•°_æœ‰åŠ¹']].describe()}"
            result = client.chat.completions.create(model="gpt-3.5-turbo", messages=[{"role": "user", "content": prompt5}])
            st.markdown(f"#### ğŸ‘¤ {name} ã•ã‚“ã¸ã®ææ¡ˆ:\n" + result.choices[0].message.content)

    except Exception as e:
        st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ï¼š{e}")
else:
    st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«æœ‰åŠ¹ãªGoogle Sheetsã®CSVãƒªãƒ³ã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")